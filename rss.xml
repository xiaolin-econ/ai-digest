<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>AI Research Digest</title>
  <link>https://xiaolin-econ.github.io/ai-digest/</link>
  <description>Curated AI research + releases</description>
  <lastBuildDate>Thu, 12 Feb 2026 06:51:14 +0000</lastBuildDate>
  <atom:link href="https://xiaolin-econ.github.io/ai-digest/rss.xml" rel="self" type="application/rss+xml" xmlns:atom="http://www.w3.org/2005/Atom"/>
  
<item>
  <title>AI Digest — Daily Summary</title>
  <link>https://xiaolin-econ.github.io/ai-digest/rss.xml</link>
  <guid>https://xiaolin-econ.github.io/ai-digest/rss.xml#summary</guid>
  <pubDate>Thu, 12 Feb 2026 06:51:14 +0000</pubDate>
  <description>As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic,…</description>
</item>

<item>
  <title>FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight</title>
  <link>https://arxiv.org/abs/2602.11136v1</link>
  <guid>https://arxiv.org/abs/2602.11136v1</guid>
  <pubDate>Wed, 11 Feb 2026 18:48:11 +0000</pubDate>
  <description>arXiv cs.AI - As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escap</description>
</item>

<item>
  <title>From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent</title>
  <link>https://arxiv.org/abs/2602.11123v1</link>
  <guid>https://arxiv.org/abs/2602.11123v1</guid>
  <pubDate>Wed, 11 Feb 2026 18:34:24 +0000</pubDate>
  <description>arXiv cs.LG - Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into execu</description>
</item>

<item>
  <title>Learning to Compose for Cross-domain Agentic Workflow Generation</title>
  <link>https://arxiv.org/abs/2602.11114v1</link>
  <guid>https://arxiv.org/abs/2602.11114v1</guid>
  <pubDate>Wed, 11 Feb 2026 18:27:22 +0000</pubDate>
  <description>arXiv cs.AI - Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically</description>
</item>

<item>
  <title>Learning to Compose for Cross-domain Agentic Workflow Generation</title>
  <link>https://arxiv.org/abs/2602.11114v1</link>
  <guid>https://arxiv.org/abs/2602.11114v1</guid>
  <pubDate>Wed, 11 Feb 2026 18:27:22 +0000</pubDate>
  <description>arXiv cs.LG - Automatically generating agentic workflows -- executable operator graphs or codes that orchestrate reasoning, verification, and repair -- has become a practical way to solve complex tasks beyond what single-pass LLM generation can reliably handle. Yet what constitutes a good workflow depends heavily on the task distribution and the available operators. Under domain shift, current systems typically</description>
</item>

<item>
  <title>GameDevBench: Evaluating Agentic Capabilities Through Game Development</title>
  <link>https://arxiv.org/abs/2602.11103v1</link>
  <guid>https://arxiv.org/abs/2602.11103v1</guid>
  <pubDate>Wed, 11 Feb 2026 18:15:11 +0000</pubDate>
  <description>arXiv cs.AI - Despite rapid progress on coding agents, progress on their multimodal counterparts has lagged behind. A key challenge is the scarcity of evaluation testbeds that combine the complexity of software development with the need for deep multimodal understanding. Game development provides such a testbed as agents must navigate large, dense codebases while manipulating intrinsically multimodal assets suc</description>
</item>

<item>
  <title>MerLin: A Discovery Engine for Photonic and Hybrid Quantum Machine Learning</title>
  <link>https://arxiv.org/abs/2602.11092v1</link>
  <guid>https://arxiv.org/abs/2602.11092v1</guid>
  <pubDate>Wed, 11 Feb 2026 18:00:01 +0000</pubDate>
  <description>arXiv cs.LG - Identifying where quantum models may offer practical benefits in near term quantum machine learning (QML) requires moving beyond isolated algorithmic proposals toward systematic and empirical exploration across models, datasets, and hardware constraints. We introduce MerLin, an open source framework designed as a discovery engine for photonic and hybrid quantum machine learning. MerLin integrates </description>
</item>

<item>
  <title>Interpretable Attention-Based Multi-Agent PPO for Latency Spike Resolution in 6G RAN Slicing</title>
  <link>https://arxiv.org/abs/2602.11076v1</link>
  <guid>https://arxiv.org/abs/2602.11076v1</guid>
  <pubDate>Wed, 11 Feb 2026 17:44:03 +0000</pubDate>
  <description>arXiv cs.AI - Sixth-generation (6G) radio access networks (RANs) must enforce strict service-level agreements (SLAs) for heterogeneous slices, yet sudden latency spikes remain difficult to diagnose and resolve with conventional deep reinforcement learning (DRL) or explainable RL (XRL). We propose \emph{Attention-Enhanced Multi-Agent Proximal Policy Optimization (AE-MAPPO)}, which integrates six specialized atte</description>
</item>

<item>
  <title>Chatting with Images for Introspective Visual Thinking</title>
  <link>https://arxiv.org/abs/2602.11073v1</link>
  <guid>https://arxiv.org/abs/2602.11073v1</guid>
  <pubDate>Wed, 11 Feb 2026 17:42:37 +0000</pubDate>
  <description>arXiv cs.AI - Current large vision-language models (LVLMs) typically rely on text-only reasoning based on a single-pass visual encoding, which often leads to loss of fine-grained visual information. Recently the proposal of &apos;&apos;thinking with images&apos;&apos; attempts to alleviate this limitation by manipulating images via external tools or code; however, the resulting visual states are often insufficiently grounded in li</description>
</item>

<item>
  <title>Chain-of-Look Spatial Reasoning for Dense Surgical Instrument Counting</title>
  <link>https://arxiv.org/abs/2602.11024v1</link>
  <guid>https://arxiv.org/abs/2602.11024v1</guid>
  <pubDate>Wed, 11 Feb 2026 16:49:37 +0000</pubDate>
  <description>arXiv cs.AI - Accurate counting of surgical instruments in Operating Rooms (OR) is a critical prerequisite for ensuring patient safety during surgery. Despite recent progress of large visual-language models and agentic AI, accurately counting such instruments remains highly challenging, particularly in dense scenarios where instruments are tightly clustered. To address this problem, we introduce Chain-of-Look, </description>
</item>

<item>
  <title>OSIL: Learning Offline Safe Imitation Policies with Safety Inferred from Non-preferred Trajectories</title>
  <link>https://arxiv.org/abs/2602.11018v1</link>
  <guid>https://arxiv.org/abs/2602.11018v1</guid>
  <pubDate>Wed, 11 Feb 2026 16:41:16 +0000</pubDate>
  <description>arXiv cs.AI - This work addresses the problem of offline safe imitation learning (IL), where the goal is to learn safe and reward-maximizing policies from demonstrations that do not have per-timestep safety cost or reward information. In many real-world domains, online learning in the environment can be risky, and specifying accurate safety costs can be difficult. However, it is often feasible to collect trajec</description>
</item>

<item>
  <title>Biases in the Blind Spot: Detecting What LLMs Fail to Mention</title>
  <link>https://arxiv.org/abs/2602.10117v1</link>
  <guid>https://arxiv.org/abs/2602.10117v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:59:56 +0000</pubDate>
  <description>arXiv cs.AI - Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipe</description>
</item>

<item>
  <title>Biases in the Blind Spot: Detecting What LLMs Fail to Mention</title>
  <link>https://arxiv.org/abs/2602.10117v1</link>
  <guid>https://arxiv.org/abs/2602.10117v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:59:56 +0000</pubDate>
  <description>arXiv cs.LG - Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipe</description>
</item>

<item>
  <title>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</title>
  <link>https://arxiv.org/abs/2602.10090v1</link>
  <guid>https://arxiv.org/abs/2602.10090v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:55:41 +0000</pubDate>
  <description>arXiv cs.AI - Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale </description>
</item>

<item>
  <title>Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</title>
  <link>https://arxiv.org/abs/2602.10090v1</link>
  <guid>https://arxiv.org/abs/2602.10090v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:55:41 +0000</pubDate>
  <description>arXiv cs.LG - Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale </description>
</item>

<item>
  <title>CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs</title>
  <link>https://arxiv.org/abs/2602.10085v1</link>
  <guid>https://arxiv.org/abs/2602.10085v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:51:39 +0000</pubDate>
  <description>arXiv cs.AI - Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. Whil</description>
</item>

<item>
  <title>Anagent For Enhancing Scientific Table &amp; Figure Analysis</title>
  <link>https://arxiv.org/abs/2602.10081v1</link>
  <guid>https://arxiv.org/abs/2602.10081v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:46:28 +0000</pubDate>
  <description>arXiv cs.AI - In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heteroge</description>
</item>

<item>
  <title>Chain of Mindset: Reasoning with Adaptive Cognitive Modes</title>
  <link>https://arxiv.org/abs/2602.10063v1</link>
  <guid>https://arxiv.org/abs/2602.10063v1</guid>
  <pubDate>Tue, 10 Feb 2026 18:31:47 +0000</pubDate>
  <description>arXiv cs.AI - Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking tha</description>
</item>

<item>
  <title>Effectiveness of Binary Autoencoders for QUBO-Based Optimization Problems</title>
  <link>https://arxiv.org/abs/2602.10037v1</link>
  <guid>https://arxiv.org/abs/2602.10037v1</guid>
  <pubDate>Tue, 10 Feb 2026 17:59:29 +0000</pubDate>
  <description>arXiv cs.LG - In black-box combinatorial optimization, objective evaluations are often expensive, so high quality solutions must be found under a limited budget. Factorization machine with quantum annealing (FMQA) builds a quadratic surrogate model from evaluated samples and optimizes it on an Ising machine. However, FMQA requires binary decision variables, and for nonbinary structures such as integer permutati</description>
</item>

<item>
  <title>Discovering High Level Patterns from Simulation Traces</title>
  <link>https://arxiv.org/abs/2602.10009v1</link>
  <guid>https://arxiv.org/abs/2602.10009v1</guid>
  <pubDate>Tue, 10 Feb 2026 17:31:39 +0000</pubDate>
  <description>arXiv cs.AI - Artificial intelligence (AI) agents embedded in environments with physics-based interaction face many challenges including reasoning, planning, summarization, and question answering. This problem is exacerbated when a human user wishes to either guide or interact with the agent in natural language. Although the use of Language Models (LMs) is the default choice, as an AI tool, they struggle with t</description>
</item>

<item>
  <title>A Collaborative Safety Shield for Safe and Efficient CAV Lane Changes in Congested On-Ramp Merging</title>
  <link>https://arxiv.org/abs/2602.10007v1</link>
  <guid>https://arxiv.org/abs/2602.10007v1</guid>
  <pubDate>Tue, 10 Feb 2026 17:30:09 +0000</pubDate>
  <description>arXiv cs.AI - Lane changing in dense traffic is a significant challenge for Connected and Autonomous Vehicles (CAVs). Existing lane change controllers primarily either ensure safety or collaboratively improve traffic efficiency, but do not consider these conflicting objectives together. To address this, we propose the Multi-Agent Safety Shield (MASS), designed using Control Barrier Functions (CBFs) to enable sa</description>
</item>

<item>
  <title>Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving</title>
  <link>https://arxiv.org/abs/2602.09018v1</link>
  <guid>https://arxiv.org/abs/2602.09018v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:59:03 +0000</pubDate>
  <description>arXiv cs.AI - Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT polici</description>
</item>

<item>
  <title>Robustness Is a Function, Not a Number: A Factorized Comprehensive Study of OOD Robustness in Vision-Based Driving</title>
  <link>https://arxiv.org/abs/2602.09018v1</link>
  <guid>https://arxiv.org/abs/2602.09018v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:59:03 +0000</pubDate>
  <description>arXiv cs.LG - Out of distribution (OOD) robustness in autonomous driving is often reduced to a single number, hiding what breaks a policy. We decompose environments along five axes: scene (rural/urban), season, weather, time (day/night), and agent mix; and measure performance under controlled $k$-factor perturbations ($k \in \{0,1,2,3\}$). Using closed loop control in VISTA, we benchmark FC, CNN, and ViT polici</description>
</item>

<item>
  <title>Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense</title>
  <link>https://arxiv.org/abs/2602.09012v1</link>
  <guid>https://arxiv.org/abs/2602.09012v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:55:33 +0000</pubDate>
  <description>arXiv cs.AI - The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like &quot;Bi</description>
</item>

<item>
  <title>Next-Gen CAPTCHAs: Leveraging the Cognitive Gap for Scalable and Diverse GUI-Agent Defense</title>
  <link>https://arxiv.org/abs/2602.09012v1</link>
  <guid>https://arxiv.org/abs/2602.09012v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:55:33 +0000</pubDate>
  <description>arXiv cs.LG - The rapid evolution of GUI-enabled agents has rendered traditional CAPTCHAs obsolete. While previous benchmarks like OpenCaptchaWorld established a baseline for evaluating multimodal agents, recent advancements in reasoning-heavy models, such as Gemini3-Pro-High and GPT-5.2-Xhigh have effectively collapsed this security barrier, achieving pass rates as high as 90% on complex logic puzzles like &quot;Bi</description>
</item>

<item>
  <title>From Obstacles to Etiquette: Robot Social Navigation with VLM-Informed Path Selection</title>
  <link>https://arxiv.org/abs/2602.09002v1</link>
  <guid>https://arxiv.org/abs/2602.09002v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:46:12 +0000</pubDate>
  <description>arXiv cs.AI - Navigating socially in human environments requires more than satisfying geometric constraints, as collision-free paths may still interfere with ongoing activities or conflict with social norms. Addressing this challenge calls for analyzing interactions between agents and incorporating common-sense reasoning into planning. This paper presents a social robot navigation framework that integrates geom</description>
</item>

<item>
  <title>InternAgent-1.5: A Unified Agentic Framework for Long-Horizon Autonomous Scientific Discovery</title>
  <link>https://arxiv.org/abs/2602.08990v1</link>
  <guid>https://arxiv.org/abs/2602.08990v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:36:06 +0000</pubDate>
  <description>arXiv cs.AI - We introduce InternAgent-1.5, a unified system designed for end-to-end scientific discovery across computational and empirical domains. The system is built on a structured architecture composed of three coordinated subsystems for generation, verification, and evolution. These subsystems are supported by foundational capabilities for deep research, solution optimization, and long horizon memory. Th</description>
</item>

<item>
  <title>StretchTime: Adaptive Time Series Forecasting via Symplectic Attention</title>
  <link>https://arxiv.org/abs/2602.08983v1</link>
  <guid>https://arxiv.org/abs/2602.08983v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:29:25 +0000</pubDate>
  <description>arXiv cs.AI - Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit &quot;time-warped&quot; dynamics where the effective flow of time decouples from the sampling index. In this </description>
</item>

<item>
  <title>StretchTime: Adaptive Time Series Forecasting via Symplectic Attention</title>
  <link>https://arxiv.org/abs/2602.08983v1</link>
  <guid>https://arxiv.org/abs/2602.08983v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:29:25 +0000</pubDate>
  <description>arXiv cs.LG - Transformer architectures have established strong baselines in time series forecasting, yet they typically rely on positional encodings that assume uniform, index-based temporal progression. However, real-world systems, from shifting financial cycles to elastic biological rhythms, frequently exhibit &quot;time-warped&quot; dynamics where the effective flow of time decouples from the sampling index. In this </description>
</item>

<item>
  <title>stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation</title>
  <link>https://arxiv.org/abs/2602.08968v1</link>
  <guid>https://arxiv.org/abs/2602.08968v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:04:22 +0000</pubDate>
  <description>arXiv cs.AI - World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardizat</description>
</item>

<item>
  <title>Learning to Coordinate via Quantum Entanglement in Multi-Agent Reinforcement Learning</title>
  <link>https://arxiv.org/abs/2602.08965v1</link>
  <guid>https://arxiv.org/abs/2602.08965v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:01:40 +0000</pubDate>
  <description>arXiv cs.LG - The inability to communicate poses a major challenge to coordination in multi-agent reinforcement learning (MARL). Prior work has explored correlating local policies via shared randomness, sometimes in the form of a correlation device, as a mechanism to assist in decentralized decision-making. In contrast, this work introduces the first framework for training MARL agents to exploit shared quantum </description>
</item>

<item>
  <title>A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents</title>
  <link>https://arxiv.org/abs/2602.08964v1</link>
  <guid>https://arxiv.org/abs/2602.08964v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:00:28 +0000</pubDate>
  <description>arXiv cs.AI - Understanding an agent&apos;s goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models&apos; internal representations. As a case study, we examine an LLM agent navigating a 2D grid world </description>
</item>

<item>
  <title>A Behavioural and Representational Evaluation of Goal-Directedness in Language Model Agents</title>
  <link>https://arxiv.org/abs/2602.08964v1</link>
  <guid>https://arxiv.org/abs/2602.08964v1</guid>
  <pubDate>Mon, 09 Feb 2026 18:00:28 +0000</pubDate>
  <description>arXiv cs.LG - Understanding an agent&apos;s goals helps explain and predict its behaviour, yet there is no established methodology for reliably attributing goals to agentic systems. We propose a framework for evaluating goal-directedness that integrates behavioural evaluation with interpretability-based analyses of models&apos; internal representations. As a case study, we examine an LLM agent navigating a 2D grid world </description>
</item>

<item>
  <title>Digital Twin and Agentic AI for Wild Fire Disaster Management: Intelligent Virtual Situation Room</title>
  <link>https://arxiv.org/abs/2602.08949v1</link>
  <guid>https://arxiv.org/abs/2602.08949v1</guid>
  <pubDate>Mon, 09 Feb 2026 17:44:52 +0000</pubDate>
  <description>arXiv cs.AI - According to the United Nations, wildfire frequency and intensity are projected to increase by approximately 14% by 2030 and 30% by 2050 due to global warming, posing critical threats to life, infrastructure, and ecosystems. Conventional disaster management frameworks rely on static simulations and passive data acquisition, hindering their ability to adapt to arbitrarily evolving wildfire episodes</description>
</item>

<item>
  <title>CoRefine: Confidence-Guided Self-Refinement for Adaptive Test-Time Compute</title>
  <link>https://arxiv.org/abs/2602.08948v1</link>
  <guid>https://arxiv.org/abs/2602.08948v1</guid>
  <pubDate>Mon, 09 Feb 2026 17:44:41 +0000</pubDate>
  <description>arXiv cs.AI - Large Language Models (LLMs) often rely on test-time scaling via parallel decoding (for example, 512 samples) to boost reasoning accuracy, but this incurs substantial compute. We introduce CoRefine, a confidence-guided self-refinement method that achieves competitive accuracy using a fraction of the tokens via a lightweight 211k-parameter Conv1D controller atop a frozen LLM. The controller consume</description>
</item>

<item>
  <title>pixelLOG: Logging of Online Gameplay for Cognitive Research</title>
  <link>https://arxiv.org/abs/2602.08941v1</link>
  <guid>https://arxiv.org/abs/2602.08941v1</guid>
  <pubDate>Mon, 09 Feb 2026 17:38:55 +0000</pubDate>
  <description>arXiv cs.AI - Traditional cognitive assessments often rely on isolated, output-focused measurements that may fail to capture the complexity of human cognition in naturalistic settings. We present pixelLOG, a high-performance data collection framework for Spigot-based Minecraft servers designed specifically for process-based cognitive research. Unlike existing frameworks tailored only for artificial intelligence</description>
</item>

<item>
  <title>AMS-HD: Hyperdimensional Computing for Real-Time and Energy-Efficient Acute Mountain Sickness Detection</title>
  <link>https://arxiv.org/abs/2602.08916v1</link>
  <guid>https://arxiv.org/abs/2602.08916v1</guid>
  <pubDate>Mon, 09 Feb 2026 17:16:13 +0000</pubDate>
  <description>arXiv cs.LG - Altitude sickness is a potentially life-threatening condition that impacts many individuals traveling to elevated altitudes. Timely detection is critical as symptoms can escalate rapidly. Early recognition enables simple interventions such as descent, oxygen, or medication, and prompt treatment can save lives by significantly lowering the risk of severe complications. Although conventional machine</description>
</item>

<item>
  <title>Gesturing Toward Abstraction: Multimodal Convention Formation in Collaborative Physical Tasks</title>
  <link>https://arxiv.org/abs/2602.08914v1</link>
  <guid>https://arxiv.org/abs/2602.08914v1</guid>
  <pubDate>Mon, 09 Feb 2026 17:13:34 +0000</pubDate>
  <description>arXiv cs.AI - A quintessential feature of human intelligence is the ability to create ad hoc conventions over time to achieve shared goals efficiently. We investigate how communication strategies evolve through repeated collaboration as people coordinate on shared procedural abstractions. To this end, we conducted an online unimodal study (n = 98) using natural language to probe abstraction hierarchies. In a fo</description>
</item>

<item>
  <title>Improving Credit Card Fraud Detection with an Optimized Explainable Boosting Machine</title>
  <link>https://arxiv.org/abs/2602.06955v1</link>
  <guid>https://arxiv.org/abs/2602.06955v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:56:17 +0000</pubDate>
  <description>arXiv cs.LG - Addressing class imbalance is a central challenge in credit card fraud detection, as it directly impacts predictive reliability in real-world financial systems. To overcome this, the study proposes an enhanced workflow based on the Explainable Boosting Machine (EBM)-a transparent, state-of-the-art implementation of the GA2M algorithm-optimized through systematic hyperparameter tuning, feature sele</description>
</item>

<item>
  <title>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</title>
  <link>https://arxiv.org/abs/2602.06949v1</link>
  <guid>https://arxiv.org/abs/2602.06949v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:49:43 +0000</pubDate>
  <description>arXiv cs.AI - Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diver</description>
</item>

<item>
  <title>DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</title>
  <link>https://arxiv.org/abs/2602.06949v1</link>
  <guid>https://arxiv.org/abs/2602.06949v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:49:43 +0000</pubDate>
  <description>arXiv cs.LG - Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diver</description>
</item>

<item>
  <title>Agentic Uncertainty Reveals Agentic Overconfidence</title>
  <link>https://arxiv.org/abs/2602.06948v1</link>
  <guid>https://arxiv.org/abs/2602.06948v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:49:35 +0000</pubDate>
  <description>arXiv cs.AI - Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination tha</description>
</item>

<item>
  <title>Agentic Uncertainty Reveals Agentic Overconfidence</title>
  <link>https://arxiv.org/abs/2602.06948v1</link>
  <guid>https://arxiv.org/abs/2602.06948v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:49:35 +0000</pubDate>
  <description>arXiv cs.LG - Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination tha</description>
</item>

<item>
  <title>Reciprocal Latent Fields for Precomputed Sound Propagation</title>
  <link>https://arxiv.org/abs/2602.06937v1</link>
  <guid>https://arxiv.org/abs/2602.06937v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:31:11 +0000</pubDate>
  <description>arXiv cs.LG - Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with </description>
</item>

<item>
  <title>Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI</title>
  <link>https://arxiv.org/abs/2602.06934v1</link>
  <guid>https://arxiv.org/abs/2602.06934v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:30:11 +0000</pubDate>
  <description>arXiv cs.AI - Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers </description>
</item>

<item>
  <title>When RL Meets Adaptive Speculative Training: A Unified Training-Serving System</title>
  <link>https://arxiv.org/abs/2602.06932v1</link>
  <guid>https://arxiv.org/abs/2602.06932v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:28:54 +0000</pubDate>
  <description>arXiv cs.LG - Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before</description>
</item>

<item>
  <title>From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers</title>
  <link>https://arxiv.org/abs/2602.06923v1</link>
  <guid>https://arxiv.org/abs/2602.06923v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:17:37 +0000</pubDate>
  <description>arXiv cs.AI - Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on &quot;world models&quot; -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous &quot;AI Physicist&quot; approaches have successfully recovered such laws, they typically rely on strong, domain-</description>
</item>

<item>
  <title>From Kepler to Newton: Inductive Biases Guide Learned World Models in Transformers</title>
  <link>https://arxiv.org/abs/2602.06923v1</link>
  <guid>https://arxiv.org/abs/2602.06923v1</guid>
  <pubDate>Fri, 06 Feb 2026 18:17:37 +0000</pubDate>
  <description>arXiv cs.LG - Can general-purpose AI architectures go beyond prediction to discover the physical laws governing the universe? True intelligence relies on &quot;world models&quot; -- causal abstractions that allow an agent to not only predict future states but understand the underlying governing dynamics. While previous &quot;AI Physicist&quot; approaches have successfully recovered such laws, they typically rely on strong, domain-</description>
</item>

<item>
  <title>TraceCoder: A Trace-Driven Multi-Agent Framework for Automated Debugging of LLM-Generated Code</title>
  <link>https://arxiv.org/abs/2602.06875v1</link>
  <guid>https://arxiv.org/abs/2602.06875v1</guid>
  <pubDate>Fri, 06 Feb 2026 16:59:48 +0000</pubDate>
  <description>arXiv cs.AI - Large Language Models (LLMs) often generate code with subtle but critical bugs, especially for complex tasks. Existing automated repair methods typically rely on superficial pass/fail signals, offering limited visibility into program behavior and hindering precise error localization. In addition, without a way to learn from prior failures, repair processes often fall into repetitive and inefficien</description>
</item>

<item>
  <title>AIRS-Bench: a Suite of Tasks for Frontier AI Research Science Agents</title>
  <link>https://arxiv.org/abs/2602.06855v1</link>
  <guid>https://arxiv.org/abs/2602.06855v1</guid>
  <pubDate>Fri, 06 Feb 2026 16:45:02 +0000</pubDate>
  <description>arXiv cs.AI - LLM agents hold significant promise for advancing scientific research. To accelerate this progress, we introduce AIRS-Bench (the AI Research Science Benchmark), a suite of 20 tasks sourced from state-of-the-art machine learning papers. These tasks span diverse domains, including language modeling, mathematics, bioinformatics, and time series forecasting. AIRS-Bench tasks assess agentic capabilitie</description>
</item>

<item>
  <title>From Features to Actions: Explainability in Traditional and Agentic AI Systems</title>
  <link>https://arxiv.org/abs/2602.06841v1</link>
  <guid>https://arxiv.org/abs/2602.06841v1</guid>
  <pubDate>Fri, 06 Feb 2026 16:34:29 +0000</pubDate>
  <description>arXiv cs.AI - Over the last decade, explainable AI has primarily focused on interpreting individual model predictions, producing post-hoc explanations that relate inputs to outputs under a fixed decision structure. Recent advances in large language models (LLMs) have enabled agentic AI systems whose behaviour unfolds over multi-step trajectories. In these settings, success and failure are determined by sequence</description>
</item>

</channel>
</rss>
