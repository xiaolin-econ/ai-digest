<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>AI Research Digest</title>
  <link>https://xiaolin-econ.github.io/ai-digest/</link>
  <description>Curated AI research + releases</description>
  <lastBuildDate>Sat, 17 Jan 2026 16:51:46 +0000</lastBuildDate>
  <atom:link href="https://xiaolin-econ.github.io/ai-digest/rss.xml" rel="self" type="application/rss+xml" xmlns:atom="http://www.w3.org/2005/Atom"/>
  
<item>
  <title>DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids</title>
  <link>https://arxiv.org/abs/2601.10715v1</link>
  <guid>https://arxiv.org/abs/2601.10715v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:59:57 +0000</pubDate>
  <description>arXiv cs.LG - We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute highe</description>
</item>

<item>
  <title>MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</title>
  <link>https://arxiv.org/abs/2601.10712v1</link>
  <guid>https://arxiv.org/abs/2601.10712v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:59:23 +0000</pubDate>
  <description>arXiv cs.AI - Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To addre</description>
</item>

<item>
  <title>High-accuracy and dimension-free sampling with diffusions</title>
  <link>https://arxiv.org/abs/2601.10708v1</link>
  <guid>https://arxiv.org/abs/2601.10708v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:58:50 +0000</pubDate>
  <description>arXiv cs.LG - Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomiall</description>
</item>

<item>
  <title>See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</title>
  <link>https://arxiv.org/abs/2601.10707v1</link>
  <guid>https://arxiv.org/abs/2601.10707v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:58:33 +0000</pubDate>
  <description>arXiv cs.LG - Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$%</description>
</item>

<item>
  <title>Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication</title>
  <link>https://arxiv.org/abs/2601.10705v1</link>
  <guid>https://arxiv.org/abs/2601.10705v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:56:54 +0000</pubDate>
  <description>arXiv cs.LG - We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent </description>
</item>

<item>
  <title>Grounding Agent Memory in Contextual Intent</title>
  <link>https://arxiv.org/abs/2601.10702v1</link>
  <guid>https://arxiv.org/abs/2601.10702v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:55:13 +0000</pubDate>
  <description>arXiv cs.AI - Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step&apos;s intent. Contextual </description>
</item>

<item>
  <title>Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis</title>
  <link>https://arxiv.org/abs/2601.10701v1</link>
  <guid>https://arxiv.org/abs/2601.10701v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:55:00 +0000</pubDate>
  <description>arXiv cs.LG - Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEP</description>
</item>

<item>
  <title>LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</title>
  <link>https://arxiv.org/abs/2601.10700v1</link>
  <guid>https://arxiv.org/abs/2601.10700v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:54:50 +0000</pubDate>
  <description>arXiv cs.AI - Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets contai</description>
</item>

<item>
  <title>The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</title>
  <link>https://arxiv.org/abs/2601.10696v1</link>
  <guid>https://arxiv.org/abs/2601.10696v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:52:59 +0000</pubDate>
  <description>arXiv cs.AI - Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, wh</description>
</item>

<item>
  <title>Data-driven stochastic reduced-order modeling of parametrized dynamical systems</title>
  <link>https://arxiv.org/abs/2601.10690v1</link>
  <guid>https://arxiv.org/abs/2601.10690v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:50:18 +0000</pubDate>
  <description>arXiv cs.LG - Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize acr</description>
</item>

<item>
  <title>On the origin of neural scaling laws: from random graphs to natural language</title>
  <link>https://arxiv.org/abs/2601.10684v1</link>
  <guid>https://arxiv.org/abs/2601.10684v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:46:09 +0000</pubDate>
  <description>arXiv cs.AI - Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with</description>
</item>

<item>
  <title>On the origin of neural scaling laws: from random graphs to natural language</title>
  <link>https://arxiv.org/abs/2601.10684v1</link>
  <guid>https://arxiv.org/abs/2601.10684v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:46:09 +0000</pubDate>
  <description>arXiv cs.LG - Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with</description>
</item>

<item>
  <title>Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems</title>
  <link>https://arxiv.org/abs/2601.10681v1</link>
  <guid>https://arxiv.org/abs/2601.10681v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:43:19 +0000</pubDate>
  <description>arXiv cs.AI - Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, cit</description>
</item>

<item>
  <title>Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</title>
  <link>https://arxiv.org/abs/2601.10679v1</link>
  <guid>https://arxiv.org/abs/2601.10679v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:42:50 +0000</pubDate>
  <description>arXiv cs.AI - Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamen</description>
</item>

<item>
  <title>Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</title>
  <link>https://arxiv.org/abs/2601.10679v1</link>
  <guid>https://arxiv.org/abs/2601.10679v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:42:50 +0000</pubDate>
  <description>arXiv cs.LG - Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamen</description>
</item>

<item>
  <title>Single-Stage Huffman Encoder for ML Compression</title>
  <link>https://arxiv.org/abs/2601.10673v1</link>
  <guid>https://arxiv.org/abs/2601.10673v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:37:56 +0000</pubDate>
  <description>arXiv cs.LG - Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensit</description>
</item>

<item>
  <title>PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution</title>
  <link>https://arxiv.org/abs/2601.10657v1</link>
  <guid>https://arxiv.org/abs/2601.10657v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:25:23 +0000</pubDate>
  <description>arXiv cs.LG - Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collab</description>
</item>

<item>
  <title>Multi-Property Synthesis</title>
  <link>https://arxiv.org/abs/2601.10651v1</link>
  <guid>https://arxiv.org/abs/2601.10651v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:18:33 +0000</pubDate>
  <description>arXiv cs.AI - We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations comp</description>
</item>

<item>
  <title>Adjusted Similarity Measures and a Violation of Expectations</title>
  <link>https://arxiv.org/abs/2601.10641v1</link>
  <guid>https://arxiv.org/abs/2601.10641v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:01:26 +0000</pubDate>
  <description>arXiv cs.LG - Adjusted similarity measures, such as Cohen&apos;s kappa for inter-rater reliability and the adjusted Rand index used to compare clustering algorithms, are a vital tool for comparing discrete labellings. These measures are intended to have the property of 0 expectation under a null distribution and maximum value 1 under maximal similarity to aid in interpretation. Measures are frequently adjusted with respect to the permutation distribution for historic and analytic reasons. There is currently renewe</description>
</item>

<item>
  <title>STEM: Scaling Transformers with Embedding Modules</title>
  <link>https://arxiv.org/abs/2601.10639v1</link>
  <guid>https://arxiv.org/abs/2601.10639v1</guid>
  <pubDate>Thu, 15 Jan 2026 18:00:27 +0000</pubDate>
  <description>arXiv cs.LG - Fine-grained sparsity promises higher parametric capacity without proportional per-token compute, but often suffers from training instability, load balancing, and communication overhead. We introduce STEM (Scaling Transformers with Embedding Modules), a static, token-indexed approach that replaces the FFN up-projection with a layer-local embedding lookup while keeping the gate and down-projection dense. This removes runtime routing, enables CPU offload with asynchronous prefetch, and decouples c</description>
</item>

<item>
  <title>Classification Imbalance as Transfer Learning</title>
  <link>https://arxiv.org/abs/2601.10630v1</link>
  <guid>https://arxiv.org/abs/2601.10630v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:49:36 +0000</pubDate>
  <description>arXiv cs.LG - Classification imbalance arises when one class is much rarer than the other. We frame this setting as transfer learning under label (prior) shift between an imbalanced source distribution induced by the observed data and a balanced target distribution under which performance is evaluated. Within this framework, we study a family of oversampling procedures that augment the training data by generating synthetic samples from an estimated minority-class distribution to roughly balance the classes, a</description>
</item>

<item>
  <title>Parametric RDT approach to computational gap of symmetric binary perceptron</title>
  <link>https://arxiv.org/abs/2601.10628v1</link>
  <guid>https://arxiv.org/abs/2601.10628v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:48:58 +0000</pubDate>
  <description>arXiv cs.LG - We study potential presence of statistical-computational gaps (SCG) in symmetric binary perceptrons (SBP) via a parametric utilization of \emph{fully lifted random duality theory} (fl-RDT) [96]. A structural change from decreasingly to arbitrarily ordered $c$-sequence (a key fl-RDT parametric component) is observed on the second lifting level and associated with \emph{satisfiability} ($α_c$) -- \emph{algorithmic} ($α_a$) constraints density threshold change thereby suggesting a potential existen</description>
</item>

<item>
  <title>Molmo2: Open Weights and Data for Vision-Language Models with Video Understanding and Grounding</title>
  <link>https://arxiv.org/abs/2601.10611v1</link>
  <guid>https://arxiv.org/abs/2601.10611v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:27:44 +0000</pubDate>
  <description>arXiv cs.AI - Today&apos;s strongest video-language models (VLMs) remain proprietary. The strongest open-weight models either rely on synthetic data from proprietary VLMs, effectively distilling from them, or do not disclose their training data or recipe. As a result, the open-source community lacks the foundations needed to improve on the state-of-the-art video (and image) language models. Crucially, many downstream applications require more than just high-level video understanding; they require grounding -- eith</description>
</item>

<item>
  <title>Procedural Fairness in Multi-Agent Bandits</title>
  <link>https://arxiv.org/abs/2601.10600v1</link>
  <guid>https://arxiv.org/abs/2601.10600v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:11:51 +0000</pubDate>
  <description>arXiv cs.AI - In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcom</description>
</item>

<item>
  <title>Procedural Fairness in Multi-Agent Bandits</title>
  <link>https://arxiv.org/abs/2601.10600v1</link>
  <guid>https://arxiv.org/abs/2601.10600v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:11:51 +0000</pubDate>
  <description>arXiv cs.LG - In the context of multi-agent multi-armed bandits (MA-MAB), fairness is often reduced to outcomes: maximizing welfare, reducing inequality, or balancing utilities. However, evidence in psychology, economics, and Rawlsian theory suggests that fairness is also about process and who gets a say in the decisions being made. We introduce a new fairness objective, procedural fairness, which provides equal decision-making power for all agents, lies in the core, and provides for proportionality in outcom</description>
</item>

<item>
  <title>ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition</title>
  <link>https://arxiv.org/abs/2601.10591v1</link>
  <guid>https://arxiv.org/abs/2601.10591v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:02:06 +0000</pubDate>
  <description>arXiv cs.AI - Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated tech</description>
</item>

<item>
  <title>ProbFM: Probabilistic Time Series Foundation Model with Uncertainty Decomposition</title>
  <link>https://arxiv.org/abs/2601.10591v1</link>
  <guid>https://arxiv.org/abs/2601.10591v1</guid>
  <pubDate>Thu, 15 Jan 2026 17:02:06 +0000</pubDate>
  <description>arXiv cs.LG - Time Series Foundation Models (TSFMs) have emerged as a promising approach for zero-shot financial forecasting, demonstrating strong transferability and data efficiency gains. However, their adoption in financial applications is hindered by fundamental limitations in uncertainty quantification: current approaches either rely on restrictive distributional assumptions, conflate different sources of uncertainty, or lack principled calibration mechanisms. While recent TSFMs employ sophisticated tech</description>
</item>

<item>
  <title>Searching for Quantum Effects in the Brain: A Bell-Type Test for Nonclassical Latent Representations in Autoencoders</title>
  <link>https://arxiv.org/abs/2601.10588v1</link>
  <guid>https://arxiv.org/abs/2601.10588v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:59:40 +0000</pubDate>
  <description>arXiv cs.LG - Whether neural information processing is entirely classical or involves quantum-mechanical elements remains an open question. Here we propose a model-agnostic, information-theoretic test of nonclassicality that bypasses microscopic assumptions and instead probes the structure of neural representations themselves. Using autoencoders as a transparent model system, we introduce a Bell-type consistency test in latent space, and ask whether decoding statistics obtained under multiple readout contexts</description>
</item>

<item>
  <title>Adversarial Evasion Attacks on Computer Vision using SHAP Values</title>
  <link>https://arxiv.org/abs/2601.10587v1</link>
  <guid>https://arxiv.org/abs/2601.10587v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:58:55 +0000</pubDate>
  <description>arXiv cs.AI - The paper introduces a white-box attack on computer vision models using SHAP values. It demonstrates how adversarial evasion attacks can compromise the performance of deep learning models by reducing output confidence or inducing misclassifications. Such attacks are particularly insidious as they can deceive the perception of an algorithm while eluding human perception due to their imperceptibility to the human eye. The proposed attack leverages SHAP values to quantify the significance of indivi</description>
</item>

<item>
  <title>Combinatorial Optimization Augmented Machine Learning</title>
  <link>https://arxiv.org/abs/2601.10583v1</link>
  <guid>https://arxiv.org/abs/2601.10583v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:55:19 +0000</pubDate>
  <description>arXiv cs.LG - Combinatorial optimization augmented machine learning (COAML) has recently emerged as a powerful paradigm for integrating predictive models with combinatorial decision-making. By embedding combinatorial optimization oracles into learning pipelines, COAML enables the construction of policies that are both data-driven and feasibility-preserving, bridging the traditions of machine learning, operations research, and stochastic optimization. This paper provides a comprehensive overview of the state o</description>
</item>

<item>
  <title>From Single to Multi-Agent Reasoning: Advancing GeneGPT for Genomics QA</title>
  <link>https://arxiv.org/abs/2601.10581v1</link>
  <guid>https://arxiv.org/abs/2601.10581v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:54:11 +0000</pubDate>
  <description>arXiv cs.AI - Comprehending genomic information is essential for biomedical research, yet extracting data from complex distributed databases remains challenging. Large language models (LLMs) offer potential for genomic Question Answering (QA) but face limitations due to restricted access to domain-specific databases. GeneGPT is the current state-of-the-art system that enhances LLMs by utilizing specialized API calls, though it is constrained by rigid API dependencies and limited adaptability. We replicate Gen</description>
</item>

<item>
  <title>Generative AI collective behavior needs an interactionist paradigm</title>
  <link>https://arxiv.org/abs/2601.10567v1</link>
  <guid>https://arxiv.org/abs/2601.10567v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:29:23 +0000</pubDate>
  <description>arXiv cs.AI - In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interact</description>
</item>

<item>
  <title>Generative AI collective behavior needs an interactionist paradigm</title>
  <link>https://arxiv.org/abs/2601.10567v1</link>
  <guid>https://arxiv.org/abs/2601.10567v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:29:23 +0000</pubDate>
  <description>arXiv cs.LG - In this article, we argue that understanding the collective behavior of agents based on large language models (LLMs) is an essential area of inquiry, with important implications in terms of risks and benefits, impacting us as a society at many levels. We claim that the distinctive nature of LLMs--namely, their initialization with extensive pre-trained knowledge and implicit social priors, together with their capability of adaptation through in-context learning--motivates the need for an interact</description>
</item>

<item>
  <title>Representation-Aware Unlearning via Activation Signatures: From Suppression to Knowledge-Signature Erasure</title>
  <link>https://arxiv.org/abs/2601.10566v1</link>
  <guid>https://arxiv.org/abs/2601.10566v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:28:14 +0000</pubDate>
  <description>arXiv cs.LG - Selective knowledge erasure from LLMs is critical for GDPR compliance and model safety, yet current unlearning methods conflate behavioral suppression with true knowledge removal, allowing latent capabilities to persist beneath surface-level refusals. In this work, we address this challenge by introducing Knowledge Immunization Framework (KIF), a representation-aware architecture that distinguishes genuine erasure from obfuscation by targeting internal activation signatures rather than surface o</description>
</item>

<item>
  <title>Kolmogorov Arnold Networks and Multi-Layer Perceptrons: A Paradigm Shift in Neural Modelling</title>
  <link>https://arxiv.org/abs/2601.10563v1</link>
  <guid>https://arxiv.org/abs/2601.10563v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:26:49 +0000</pubDate>
  <description>arXiv cs.LG - The research undertakes a comprehensive comparative analysis of Kolmogorov-Arnold Networks (KAN) and Multi-Layer Perceptrons (MLP), highlighting their effectiveness in solving essential computational challenges like nonlinear function approximation, time-series prediction, and multivariate classification. Rooted in Kolmogorov&apos;s representation theorem, KANs utilize adaptive spline-based activation functions and grid-based structures, providing a transformative approach compared to traditional neu</description>
</item>

<item>
  <title>Process-Guided Concept Bottleneck Model</title>
  <link>https://arxiv.org/abs/2601.10562v1</link>
  <guid>https://arxiv.org/abs/2601.10562v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:25:55 +0000</pubDate>
  <description>arXiv cs.AI - Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains</description>
</item>

<item>
  <title>Process-Guided Concept Bottleneck Model</title>
  <link>https://arxiv.org/abs/2601.10562v1</link>
  <guid>https://arxiv.org/abs/2601.10562v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:25:55 +0000</pubDate>
  <description>arXiv cs.LG - Concept Bottleneck Models (CBMs) improve the explainability of black-box Deep Learning (DL) by introducing intermediate semantic concepts. However, standard CBMs often overlook domain-specific relationships and causal mechanisms, and their dependence on complete concept labels limits applicability in scientific domains where supervision is sparse but processes are well defined. To address this, we propose the Process-Guided Concept Bottleneck Model (PG-CBM), an extension of CBMs which constrains</description>
</item>

<item>
  <title>Learning Latency-Aware Orchestration for Parallel Multi-Agent Systems</title>
  <link>https://arxiv.org/abs/2601.10560v1</link>
  <guid>https://arxiv.org/abs/2601.10560v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:23:53 +0000</pubDate>
  <description>arXiv cs.AI - Multi-agent systems (MAS) enable complex reasoning by coordinating multiple agents, but often incur high inference latency due to multi-step execution and repeated model invocations, severely limiting their scalability and usability in time-sensitive scenarios. Most existing approaches primarily optimize task performance and inference cost, and explicitly or implicitly assume sequential execution, making them less optimal for controlling latency under parallel execution. In this work, we investi</description>
</item>

<item>
  <title>Defending Large Language Models Against Jailbreak Attacks via In-Decoding Safety-Awareness Probing</title>
  <link>https://arxiv.org/abs/2601.10543v1</link>
  <guid>https://arxiv.org/abs/2601.10543v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:09:10 +0000</pubDate>
  <description>arXiv cs.AI - Large language models (LLMs) have achieved impressive performance across natural language tasks and are increasingly deployed in real-world applications. Despite extensive safety alignment efforts, recent studies show that such alignment is often shallow and remains vulnerable to jailbreak attacks. Existing defense mechanisms, including decoding-based constraints and post-hoc content detectors, struggle against sophisticated jailbreaks, often intervening robust detection or excessively degrading</description>
</item>

<item>
  <title>Mixtures of Transparent Local Models</title>
  <link>https://arxiv.org/abs/2601.10541v1</link>
  <guid>https://arxiv.org/abs/2601.10541v1</guid>
  <pubDate>Thu, 15 Jan 2026 16:05:20 +0000</pubDate>
  <description>arXiv cs.LG - The predominance of machine learning models in many spheres of human activity has led to a growing demand for their transparency. The transparency of models makes it possible to discern some factors, such as security or non-discrimination. In this paper, we propose a mixture of transparent local models as an alternative solution for designing interpretable (or transparent) models. Our approach is designed for the situations where a simple and transparent function is suitable for modeling the lab</description>
</item>

<item>
  <title>CoGen: Creation of Reusable UI Components in Figma via Textual Commands</title>
  <link>https://arxiv.org/abs/2601.10536v1</link>
  <guid>https://arxiv.org/abs/2601.10536v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:57:59 +0000</pubDate>
  <description>arXiv cs.LG - The evolution of User Interface design has emphasized the need for efficient, reusable, and editable components to ensure an efficient design process. This research introduces CoGen, a system that uses machine learning techniques to generate reusable UI components directly in Figma, one of the most popular UI design tools. Addressing gaps in current systems, CoGen focuses on creating atomic components such as buttons, labels, and input fields using structured JSON and natural language prompts.
 </description>
</item>

<item>
  <title>Coarsening Causal DAG Models</title>
  <link>https://arxiv.org/abs/2601.10531v1</link>
  <guid>https://arxiv.org/abs/2601.10531v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:56:20 +0000</pubDate>
  <description>arXiv cs.LG - Directed acyclic graphical (DAG) models are a powerful tool for representing causal relationships among jointly distributed random variables, especially concerning data from across different experimental settings. However, it is not always practical or desirable to estimate a causal model at the granularity of given features in a particular dataset. There is a growing body of research on causal abstraction to address such problems. We contribute to this line of research by (i) providing novel gr</description>
</item>

<item>
  <title>A Safety Report on GPT-5.2, Gemini 3 Pro, Qwen3-VL, Doubao 1.8, Grok 4.1 Fast, Nano Banana Pro, and Seedream 4.5</title>
  <link>https://arxiv.org/abs/2601.10527v1</link>
  <guid>https://arxiv.org/abs/2601.10527v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:52:52 +0000</pubDate>
  <description>arXiv cs.AI - The rapid evolution of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has produced substantial gains in reasoning, perception, and generative capability across language and vision. However, whether these advances yield commensurate improvements in safety remains unclear, in part due to fragmented evaluation practices limited to single modalities or threat models. In this report, we present an integrated safety evaluation of 7 frontier models: GPT-5.2, Gemini 3 Pro, Qwe</description>
</item>

<item>
  <title>Diagnosing Generalization Failures in Fine-Tuned LLMs: A Cross-Architectural Study on Phishing Detection</title>
  <link>https://arxiv.org/abs/2601.10524v1</link>
  <guid>https://arxiv.org/abs/2601.10524v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:51:24 +0000</pubDate>
  <description>arXiv cs.AI - The practice of fine-tuning Large Language Models (LLMs) has achieved state-of-the-art performance on specialized tasks, yet diagnosing why these models become brittle and fail to generalize remains a critical open problem. To address this, we introduce and apply a multi-layered diagnostic framework to a cross-architectural study. We fine-tune Llama 3.1 8B, Gemma 2 9B, and Mistral models on a high-stakes phishing detection task and use SHAP analysis and mechanistic interpretability to uncover th</description>
</item>

<item>
  <title>Breaking Up with Normatively Monolithic Agency with GRACE: A Reason-Based Neuro-Symbolic Architecture for Safe and Ethical AI Alignment</title>
  <link>https://arxiv.org/abs/2601.10520v1</link>
  <guid>https://arxiv.org/abs/2601.10520v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:47:38 +0000</pubDate>
  <description>arXiv cs.AI - As AI agents become increasingly autonomous, widely deployed in consequential contexts, and efficacious in bringing about real-world impacts, ensuring that their decisions are not only instrumentally effective but also normatively aligned has become critical. We introduce a neuro-symbolic reason-based containment architecture, Governor for Reason-Aligned ContainmEnt (GRACE), that decouples normative reasoning from instrumental decision-making and can contain AI agents of virtually any design. GR</description>
</item>

<item>
  <title>SatMap: Revisiting Satellite Maps as Prior for Online HD Map Construction</title>
  <link>https://arxiv.org/abs/2601.10512v1</link>
  <guid>https://arxiv.org/abs/2601.10512v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:39:27 +0000</pubDate>
  <description>arXiv cs.AI - Online high-definition (HD) map construction is an essential part of a safe and robust end-to-end autonomous driving (AD) pipeline. Onboard camera-based approaches suffer from limited depth perception and degraded accuracy due to occlusion. In this work, we propose SatMap, an online vectorized HD map estimation method that integrates satellite maps with multi-view camera observations and directly predicts a vectorized HD map for downstream prediction and planning modules. Our method leverages la</description>
</item>

<item>
  <title>Scalable Algorithms for Approximate DNF Model Counting</title>
  <link>https://arxiv.org/abs/2601.10511v1</link>
  <guid>https://arxiv.org/abs/2601.10511v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:38:47 +0000</pubDate>
  <description>arXiv cs.AI - Model counting of Disjunctive Normal Form (DNF) formulas is a critical problem in applications such as probabilistic inference and network reliability. For example, it is often used for query evaluation in probabilistic databases. Due to the computational intractability of exact DNF counting, there has been a line of research into a variety of approximation algorithms. These include Monte Carlo approaches such as the classical algorithms of Karp, Luby, and Madras (1989), as well as methods based</description>
</item>

<item>
  <title>Projected Microbatch Accumulation yields reference-free proximal policy updates for reinforcement learning</title>
  <link>https://arxiv.org/abs/2601.10498v1</link>
  <guid>https://arxiv.org/abs/2601.10498v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:16:15 +0000</pubDate>
  <description>arXiv cs.AI - This note introduces Projected Microbatch Accumulation (PROMA), a proximal policy update method for large language model fine-tuning. PROMA accumulates policy gradients across microbatches by projecting out sequence-wise gradient components before microbatch aggregation. The projection is applied layer-wise during the backward pass, enabling efficient implementation without additional forward or backward passes. Empirically, PROMA enforces tighter control of local KL divergence than GRPO, result</description>
</item>

<item>
  <title>Model See, Model Do? Exposure-Aware Evaluation of Bug-vs-Fix Preference in Code LLMs</title>
  <link>https://arxiv.org/abs/2601.10496v1</link>
  <guid>https://arxiv.org/abs/2601.10496v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:14:29 +0000</pubDate>
  <description>arXiv cs.AI - Large language models are increasingly used for code generation and debugging, but their outputs can still contain bugs, that originate from training data. Distinguishing whether an LLM prefers correct code, or a familiar incorrect version might be influenced by what it&apos;s been exposed to during training. We introduce an exposure-aware evaluation framework that quantifies how prior exposure to buggy versus fixed code influences a model&apos;s preference. Using the ManySStuBs4J benchmark, we apply Data</description>
</item>

<item>
  <title>Panning for Gold: Expanding Domain-Specific Knowledge Graphs with General Knowledge</title>
  <link>https://arxiv.org/abs/2601.10485v1</link>
  <guid>https://arxiv.org/abs/2601.10485v1</guid>
  <pubDate>Thu, 15 Jan 2026 15:06:56 +0000</pubDate>
  <description>arXiv cs.AI - Domain-specific knowledge graphs (DKGs) often lack coverage compared to general knowledge graphs (GKGs). To address this, we introduce Domain-specific Knowledge Graph Fusion (DKGF), a novel task that enriches DKGs by integrating relevant facts from GKGs. DKGF faces two key challenges: high ambiguity in domain relevance and misalignment in knowledge granularity across graphs. We propose ExeFuse, a simple yet effective Fact-as-Program paradigm. It treats each GKG fact as a latent semantic program,</description>
</item>

</channel>
</rss>
