<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0">
<channel>
  <title>AI Research Digest</title>
  <link>https://xiaolin-econ.github.io/ai-digest/</link>
  <description>Curated AI research + releases</description>
  <lastBuildDate>Fri, 30 Jan 2026 06:36:19 +0000</lastBuildDate>
  <atom:link href="https://xiaolin-econ.github.io/ai-digest/rss.xml" rel="self" type="application/rss+xml" xmlns:atom="http://www.w3.org/2005/Atom"/>
  
<item>
  <title>AI Digest — Daily Summary</title>
  <link>https://xiaolin-econ.github.io/ai-digest/rss.xml</link>
  <guid>https://xiaolin-econ.github.io/ai-digest/rss.xml#summary</guid>
  <pubDate>Fri, 30 Jan 2026 06:36:19 +0000</pubDate>
  <description>Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity…</description>
</item>

<item>
  <title>RedSage: A Cybersecurity Generalist LLM</title>
  <link>https://arxiv.org/abs/2601.22159v1</link>
  <guid>https://arxiv.org/abs/2601.22159v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:59:57 +0000</pubDate>
  <description>arXiv cs.AI - Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, </description>
</item>

<item>
  <title>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</title>
  <link>https://arxiv.org/abs/2601.22156v1</link>
  <guid>https://arxiv.org/abs/2601.22156v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:59:53 +0000</pubDate>
  <description>arXiv cs.AI - Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RN</description>
</item>

<item>
  <title>Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</title>
  <link>https://arxiv.org/abs/2601.22156v1</link>
  <guid>https://arxiv.org/abs/2601.22156v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:59:53 +0000</pubDate>
  <description>arXiv cs.LG - Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RN</description>
</item>

<item>
  <title>Exploring Reasoning Reward Model for Agents</title>
  <link>https://arxiv.org/abs/2601.22154v1</link>
  <guid>https://arxiv.org/abs/2601.22154v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:59:52 +0000</pubDate>
  <description>arXiv cs.AI - Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a mult</description>
</item>

<item>
  <title>DynaWeb: Model-Based Reinforcement Learning of Web Agents</title>
  <link>https://arxiv.org/abs/2601.22149v1</link>
  <guid>https://arxiv.org/abs/2601.22149v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:59:07 +0000</pubDate>
  <description>arXiv cs.AI - The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a pr</description>
</item>

<item>
  <title>StepShield: When, Not Whether to Intervene on Rogue Agents</title>
  <link>https://arxiv.org/abs/2601.22136v1</link>
  <guid>https://arxiv.org/abs/2601.22136v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:55:46 +0000</pubDate>
  <description>arXiv cs.AI - Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, </description>
</item>

<item>
  <title>StepShield: When, Not Whether to Intervene on Rogue Agents</title>
  <link>https://arxiv.org/abs/2601.22136v1</link>
  <guid>https://arxiv.org/abs/2601.22136v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:55:46 +0000</pubDate>
  <description>arXiv cs.LG - Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, </description>
</item>

<item>
  <title>Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</title>
  <link>https://arxiv.org/abs/2601.22132v1</link>
  <guid>https://arxiv.org/abs/2601.22132v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:52:54 +0000</pubDate>
  <description>arXiv cs.LG - Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a com</description>
</item>

<item>
  <title>World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems</title>
  <link>https://arxiv.org/abs/2601.22130v1</link>
  <guid>https://arxiv.org/abs/2601.22130v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:51:54 +0000</pubDate>
  <description>arXiv cs.AI - Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observa</description>
</item>

<item>
  <title>SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</title>
  <link>https://arxiv.org/abs/2601.22129v1</link>
  <guid>https://arxiv.org/abs/2601.22129v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:50:29 +0000</pubDate>
  <description>arXiv cs.AI - Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to genera</description>
</item>

<item>
  <title>SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</title>
  <link>https://arxiv.org/abs/2601.22129v1</link>
  <guid>https://arxiv.org/abs/2601.22129v1</guid>
  <pubDate>Thu, 29 Jan 2026 18:50:29 +0000</pubDate>
  <description>arXiv cs.LG - Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to genera</description>
</item>

<item>
  <title>Vision-DeepResearch: Incentivizing DeepResearch Capability in Multimodal Large Language Models</title>
  <link>https://arxiv.org/abs/2601.22060v1</link>
  <guid>https://arxiv.org/abs/2601.22060v1</guid>
  <pubDate>Thu, 29 Jan 2026 17:58:40 +0000</pubDate>
  <description>arXiv cs.AI - Multimodal large language models (MLLMs) have achieved remarkable success across a broad range of vision tasks. However, constrained by the capacity of their internal world knowledge, prior work has proposed augmenting MLLMs by ``reasoning-then-tool-call&apos;&apos; for visual and textual search engines to obtain substantial gains on tasks requiring extensive factual information. However, these approaches t</description>
</item>

<item>
  <title>SIA: Symbolic Interpretability for Anticipatory Deep Reinforcement Learning in Network Control</title>
  <link>https://arxiv.org/abs/2601.22044v1</link>
  <guid>https://arxiv.org/abs/2601.22044v1</guid>
  <pubDate>Thu, 29 Jan 2026 17:46:46 +0000</pubDate>
  <description>arXiv cs.AI - Deep reinforcement learning (DRL) promises adaptive control for future mobile networks but conventional agents remain reactive: they act on past and current measurements and cannot leverage short-term forecasts of exogenous KPIs such as bandwidth. Augmenting agents with predictions can overcome this temporal myopia, yet uptake in networking is scarce because forecast-aware agents act as closed-box</description>
</item>

<item>
  <title>Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems</title>
  <link>https://arxiv.org/abs/2601.22041v1</link>
  <guid>https://arxiv.org/abs/2601.22041v1</guid>
  <pubDate>Thu, 29 Jan 2026 17:45:41 +0000</pubDate>
  <description>arXiv cs.AI - Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignm</description>
</item>

<item>
  <title>Learning to Communicate Across Modalities: Perceptual Heterogeneity in Multi-Agent Systems</title>
  <link>https://arxiv.org/abs/2601.22041v1</link>
  <guid>https://arxiv.org/abs/2601.22041v1</guid>
  <pubDate>Thu, 29 Jan 2026 17:45:41 +0000</pubDate>
  <description>arXiv cs.LG - Emergent communication offers insight into how agents develop shared structured representations, yet most research assumes homogeneous modalities or aligned representational spaces, overlooking the perceptual heterogeneity of real-world settings. We study a heterogeneous multi-step binary communication game where agents differ in modality and lack perceptual grounding. Despite perceptual misalignm</description>
</item>

<item>
  <title>Optimizing Agentic Workflows using Meta-tools</title>
  <link>https://arxiv.org/abs/2601.22037v1</link>
  <guid>https://arxiv.org/abs/2601.22037v1</guid>
  <pubDate>Thu, 29 Jan 2026 17:43:08 +0000</pubDate>
  <description>arXiv cs.AI - Agentic AI enables LLM to dynamically reason, plan, and interact with tools to solve complex tasks. However, agentic workflows often require many iterative reasoning steps and tool invocations, leading to significant operational expense, end-to-end latency and failures due to hallucinations. This work introduces Agent Workflow Optimization (AWO), a framework that identifies and optimizes redundant</description>
</item>

<item>
  <title>Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)</title>
  <link>https://arxiv.org/abs/2601.20843v1</link>
  <guid>https://arxiv.org/abs/2601.20843v1</guid>
  <pubDate>Wed, 28 Jan 2026 18:45:39 +0000</pubDate>
  <description>arXiv cs.AI - This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficie</description>
</item>

<item>
  <title>MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents</title>
  <link>https://arxiv.org/abs/2601.20831v1</link>
  <guid>https://arxiv.org/abs/2601.20831v1</guid>
  <pubDate>Wed, 28 Jan 2026 18:31:17 +0000</pubDate>
  <description>arXiv cs.AI - Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we pr</description>
</item>

<item>
  <title>Demystifying Prediction Powered Inference</title>
  <link>https://arxiv.org/abs/2601.20819v1</link>
  <guid>https://arxiv.org/abs/2601.20819v1</guid>
  <pubDate>Wed, 28 Jan 2026 18:16:02 +0000</pubDate>
  <description>arXiv cs.LG - Machine learning predictions are increasingly used to supplement incomplete or costly-to-measure outcomes in fields such as biomedical research, environmental science, and social science. However, treating predictions as ground truth introduces bias while ignoring them wastes valuable information. Prediction-Powered Inference (PPI) offers a principled framework that leverages predictions from larg</description>
</item>

<item>
  <title>SERA: Soft-Verified Efficient Repository Agents</title>
  <link>https://arxiv.org/abs/2601.20789v1</link>
  <guid>https://arxiv.org/abs/2601.20789v1</guid>
  <pubDate>Wed, 28 Jan 2026 17:27:08 +0000</pubDate>
  <description>arXiv cs.LG - Open-weight coding agents should hold a fundamental advantage over closed-source systems: they can be specialized to private codebases, encoding repository-specific information directly in their weights. Yet the cost and complexity of training has kept this advantage theoretical. We show it is now practical. We present Soft-Verified Efficient Repository Agents (SERA), an efficient method for train</description>
</item>

<item>
  <title>COMET-SG1: Lightweight Autoregressive Regressor for Edge and Embedded AI</title>
  <link>https://arxiv.org/abs/2601.20772v1</link>
  <guid>https://arxiv.org/abs/2601.20772v1</guid>
  <pubDate>Wed, 28 Jan 2026 16:59:56 +0000</pubDate>
  <description>arXiv cs.LG - COMET-SG1 is a lightweight, stability-oriented autoregressive regression model designed for time-series prediction on edge and embedded AI systems. Unlike recurrent neural networks or transformer-based sequence models, COMET-SG1 operates through linear behavior-space encoding, memory-anchored transition estimation, and deterministic state updates. This structure prioritizes bounded long-horizon be</description>
</item>

<item>
  <title>Li-ViP3D++: Query-Gated Deformable Camera-LiDAR Fusion for End-to-End Perception and Trajectory Prediction</title>
  <link>https://arxiv.org/abs/2601.20720v1</link>
  <guid>https://arxiv.org/abs/2601.20720v1</guid>
  <pubDate>Wed, 28 Jan 2026 15:53:32 +0000</pubDate>
  <description>arXiv cs.AI - End-to-end perception and trajectory prediction from raw sensor data is one of the key capabilities for autonomous driving. Modular pipelines restrict information flow and can amplify upstream errors. Recent query-based, fully differentiable perception-and-prediction (PnP) models mitigate these issues, yet the complementarity of cameras and LiDAR in the query-space has not been sufficiently explor</description>
</item>

<item>
  <title>Adapting the Behavior of Reinforcement Learning Agents to Changing Action Spaces and Reward Functions</title>
  <link>https://arxiv.org/abs/2601.20714v1</link>
  <guid>https://arxiv.org/abs/2601.20714v1</guid>
  <pubDate>Wed, 28 Jan 2026 15:46:51 +0000</pubDate>
  <description>arXiv cs.AI - Reinforcement Learning (RL) agents often struggle in real-world applications where environmental conditions are non-stationary, particularly when reward functions shift or the available action space expands. This paper introduces MORPHIN, a self-adaptive Q-learning framework that enables on-the-fly adaptation without full retraining. By integrating concept drift detection with dynamic adjustments </description>
</item>

<item>
  <title>SONIC: Spectral Oriented Neural Invariant Convolutions</title>
  <link>https://arxiv.org/abs/2601.19884v1</link>
  <guid>https://arxiv.org/abs/2601.19884v1</guid>
  <pubDate>Tue, 27 Jan 2026 18:51:11 +0000</pubDate>
  <description>arXiv cs.LG - Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these li</description>
</item>

<item>
  <title>Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals</title>
  <link>https://arxiv.org/abs/2601.19810v1</link>
  <guid>https://arxiv.org/abs/2601.19810v1</guid>
  <pubDate>Tue, 27 Jan 2026 17:10:29 +0000</pubDate>
  <description>arXiv cs.AI - Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream t</description>
</item>

<item>
  <title>Unsupervised Learning of Efficient Exploration: Pre-training Adaptive Policies via Self-Imposed Goals</title>
  <link>https://arxiv.org/abs/2601.19810v1</link>
  <guid>https://arxiv.org/abs/2601.19810v1</guid>
  <pubDate>Tue, 27 Jan 2026 17:10:29 +0000</pubDate>
  <description>arXiv cs.LG - Unsupervised pre-training can equip reinforcement learning agents with prior knowledge and accelerate learning in downstream tasks. A promising direction, grounded in human development, investigates agents that learn by setting and pursuing their own goals. The core challenge lies in how to effectively generate, select, and learn from such goals. Our focus is on broad distributions of downstream t</description>
</item>

<item>
  <title>Component-Aware Pruning Framework for Neural Network Controllers via Gradient-Based Importance Estimation</title>
  <link>https://arxiv.org/abs/2601.19794v1</link>
  <guid>https://arxiv.org/abs/2601.19794v1</guid>
  <pubDate>Tue, 27 Jan 2026 16:53:19 +0000</pubDate>
  <description>arXiv cs.LG - The transition from monolithic to multi-component neural architectures in advanced neural network controllers poses substantial challenges due to the high computational complexity of the latter. Conventional model compression techniques for complexity reduction, such as structured pruning based on norm-based metrics to estimate the relative importance of distinct parameter groups, often fail to ca</description>
</item>

<item>
  <title>CASTER: Breaking the Cost-Performance Barrier in Multi-Agent Orchestration via Context-Aware Strategy for Task Efficient Routing</title>
  <link>https://arxiv.org/abs/2601.19793v1</link>
  <guid>https://arxiv.org/abs/2601.19793v1</guid>
  <pubDate>Tue, 27 Jan 2026 16:52:47 +0000</pubDate>
  <description>arXiv cs.AI - Graph-based Multi-Agent Systems (MAS) enable complex cyclic workflows but suffer from inefficient static model allocation, where deploying strong models uniformly wastes computation on trivial sub-tasks. We propose CASTER (Context-Aware Strategy for Task Efficient Routing), a lightweight router for dynamic model selection in graph-based MAS. CASTER employs a Dual-Signal Router that combines semant</description>
</item>

<item>
  <title>LVLMs and Humans Ground Differently in Referential Communication</title>
  <link>https://arxiv.org/abs/2601.19792v1</link>
  <guid>https://arxiv.org/abs/2601.19792v1</guid>
  <pubDate>Tue, 27 Jan 2026 16:52:20 +0000</pubDate>
  <description>arXiv cs.AI - For generative AI agents to partner effectively with human users, the ability to accurately predict human intent is critical. But this ability to collaborate remains limited by a critical deficit: an inability to model common ground. Here, we present a referential communication experiment with a factorial design involving director-matcher pairs (human-human, human-AI, AI-human, and AI-AI) that int</description>
</item>

<item>
  <title>Reimagining Peer Review Process Through Multi-Agent Mechanism Design</title>
  <link>https://arxiv.org/abs/2601.19778v1</link>
  <guid>https://arxiv.org/abs/2601.19778v1</guid>
  <pubDate>Tue, 27 Jan 2026 16:43:11 +0000</pubDate>
  <description>arXiv cs.AI - The software engineering research community faces a systemic crisis: peer review is failing under growing submissions, misaligned incentives, and reviewer fatigue. Community surveys reveal that researchers perceive the process as &quot;broken.&quot; This position paper argues that these dysfunctions are mechanism design failures amenable to computational solutions. We propose modeling the research community</description>
</item>

<item>
  <title>Agentic Design Patterns: A System-Theoretic Framework</title>
  <link>https://arxiv.org/abs/2601.19752v1</link>
  <guid>https://arxiv.org/abs/2601.19752v1</guid>
  <pubDate>Tue, 27 Jan 2026 16:14:08 +0000</pubDate>
  <description>arXiv cs.AI - With the development of foundation model (FM), agentic AI systems are getting more attention, yet their inherent issues like hallucination and poor reasoning, coupled with the frequent ad-hoc nature of system design, lead to unreliable and brittle applications. Existing efforts to characterise agentic design patterns often lack a rigorous systems-theoretic foundation, resulting in high-level or co</description>
</item>

<item>
  <title>Veri-Sure: A Contract-Aware Multi-Agent Framework with Temporal Tracing and Formal Verification for Correct RTL Code Generation</title>
  <link>https://arxiv.org/abs/2601.19747v1</link>
  <guid>https://arxiv.org/abs/2601.19747v1</guid>
  <pubDate>Tue, 27 Jan 2026 16:10:23 +0000</pubDate>
  <description>arXiv cs.AI - In the rapidly evolving field of Electronic Design Automation (EDA), the deployment of Large Language Models (LLMs) for Register-Transfer Level (RTL) design has emerged as a promising direction. However, silicon-grade correctness remains bottlenecked by: (i) limited test coverage and reliability of simulation-centric evaluation, (ii) regressions and repair hallucinations introduced by iterative de</description>
</item>

<item>
  <title>Quantum Circuit Pre-Synthesis: Learning Local Edits to Reduce $T$-count</title>
  <link>https://arxiv.org/abs/2601.19738v1</link>
  <guid>https://arxiv.org/abs/2601.19738v1</guid>
  <pubDate>Tue, 27 Jan 2026 15:58:05 +0000</pubDate>
  <description>arXiv cs.AI - Compiling quantum circuits into Clifford+$T$ gates is a central task for fault-tolerant quantum computing using stabilizer codes. In the near term, $T$ gates will dominate the cost of fault tolerant implementations, and any reduction in the number of such expensive gates could mean the difference between being able to run a circuit or not. While exact synthesis is exponentially hard in the number </description>
</item>

<item>
  <title>Quantum Circuit Pre-Synthesis: Learning Local Edits to Reduce $T$-count</title>
  <link>https://arxiv.org/abs/2601.19738v1</link>
  <guid>https://arxiv.org/abs/2601.19738v1</guid>
  <pubDate>Tue, 27 Jan 2026 15:58:05 +0000</pubDate>
  <description>arXiv cs.LG - Compiling quantum circuits into Clifford+$T$ gates is a central task for fault-tolerant quantum computing using stabilizer codes. In the near term, $T$ gates will dominate the cost of fault tolerant implementations, and any reduction in the number of such expensive gates could mean the difference between being able to run a circuit or not. While exact synthesis is exponentially hard in the number </description>
</item>

<item>
  <title>ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models</title>
  <link>https://arxiv.org/abs/2601.18796v1</link>
  <guid>https://arxiv.org/abs/2601.18796v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:58:46 +0000</pubDate>
  <description>arXiv cs.AI - Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. W</description>
</item>

<item>
  <title>ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models</title>
  <link>https://arxiv.org/abs/2601.18796v1</link>
  <guid>https://arxiv.org/abs/2601.18796v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:58:46 +0000</pubDate>
  <description>arXiv cs.LG - Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. W</description>
</item>

<item>
  <title>MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data</title>
  <link>https://arxiv.org/abs/2601.18792v1</link>
  <guid>https://arxiv.org/abs/2601.18792v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:55:44 +0000</pubDate>
  <description>arXiv cs.LG - Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalog</description>
</item>

<item>
  <title>Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets</title>
  <link>https://arxiv.org/abs/2601.18791v1</link>
  <guid>https://arxiv.org/abs/2601.18791v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:55:28 +0000</pubDate>
  <description>arXiv cs.AI - We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing &apos;glottosets&apos; from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity a</description>
</item>

<item>
  <title>Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets</title>
  <link>https://arxiv.org/abs/2601.18791v1</link>
  <guid>https://arxiv.org/abs/2601.18791v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:55:28 +0000</pubDate>
  <description>arXiv cs.LG - We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing &apos;glottosets&apos; from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity a</description>
</item>

<item>
  <title>$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks</title>
  <link>https://arxiv.org/abs/2601.18754v1</link>
  <guid>https://arxiv.org/abs/2601.18754v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:25:07 +0000</pubDate>
  <description>arXiv cs.AI - Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditi</description>
</item>

<item>
  <title>Capturing P: On the Expressive Power and Efficient Evaluation of Boolean Retrieval</title>
  <link>https://arxiv.org/abs/2601.18747v1</link>
  <guid>https://arxiv.org/abs/2601.18747v1</guid>
  <pubDate>Mon, 26 Jan 2026 18:07:40 +0000</pubDate>
  <description>arXiv cs.AI - Modern information retrieval is transitioning from simple document filtering to complex, neuro-symbolic reasoning workflows. However, current retrieval architectures face a fundamental efficiency dilemma when handling the rigorous logical and arithmetic constraints required by this new paradigm. Standard iterator-based engines (Document-at-a-Time) do not natively support complex, nested logic grap</description>
</item>

<item>
  <title>Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems</title>
  <link>https://arxiv.org/abs/2601.18735v1</link>
  <guid>https://arxiv.org/abs/2601.18735v1</guid>
  <pubDate>Mon, 26 Jan 2026 17:58:53 +0000</pubDate>
  <description>arXiv cs.AI - Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We</description>
</item>

<item>
  <title>Why Keep Your Doubts to Yourself? Trading Visual Uncertainties in Multi-Agent Bandit Systems</title>
  <link>https://arxiv.org/abs/2601.18735v1</link>
  <guid>https://arxiv.org/abs/2601.18735v1</guid>
  <pubDate>Mon, 26 Jan 2026 17:58:53 +0000</pubDate>
  <description>arXiv cs.LG - Vision-Language Models (VLMs) enable powerful multi-agent systems, but scaling them is economically unsustainable: coordinating heterogeneous agents under information asymmetry often spirals costs. Existing paradigms, such as Mixture-of-Agents and knowledge-based routers, rely on heuristic proxies that ignore costs and collapse uncertainty structure, leading to provably suboptimal coordination. We</description>
</item>

<item>
  <title>Advances and Innovations in the Multi-Agent Robotic System (MARS) Challenge</title>
  <link>https://arxiv.org/abs/2601.18733v1</link>
  <guid>https://arxiv.org/abs/2601.18733v1</guid>
  <pubDate>Mon, 26 Jan 2026 17:56:19 +0000</pubDate>
  <description>arXiv cs.AI - Recent advancements in multimodal large language models and vision-languageaction models have significantly driven progress in Embodied AI. As the field transitions toward more complex task scenarios, multi-agent system frameworks are becoming essential for achieving scalable, efficient, and collaborative solutions. This shift is fueled by three primary factors: increasing agent capabilities, enha</description>
</item>

<item>
  <title>Optimal Use of Preferences in Artificial Intelligence Algorithms</title>
  <link>https://arxiv.org/abs/2601.18732v1</link>
  <guid>https://arxiv.org/abs/2601.18732v1</guid>
  <pubDate>Mon, 26 Jan 2026 17:55:56 +0000</pubDate>
  <description>arXiv cs.AI - Machine learning systems embed preferences either in training losses or through post-processing of calibrated predictions. Applying information design methods from Strack and Yang (2024), this paper provides decision problem agnostic conditions under which separation training preference free and applying preferences ex post is optimal. Unlike prior work that requires specifying downstream objectiv</description>
</item>

<item>
  <title>Auto-Regressive Masked Diffusion Models</title>
  <link>https://arxiv.org/abs/2601.16971v1</link>
  <guid>https://arxiv.org/abs/2601.16971v1</guid>
  <pubDate>Fri, 23 Jan 2026 18:42:30 +0000</pubDate>
  <description>arXiv cs.LG - Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel </description>
</item>

<item>
  <title>Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</title>
  <link>https://arxiv.org/abs/2601.16965v1</link>
  <guid>https://arxiv.org/abs/2601.16965v1</guid>
  <pubDate>Fri, 23 Jan 2026 18:33:45 +0000</pubDate>
  <description>arXiv cs.AI - Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial informat</description>
</item>

<item>
  <title>AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems</title>
  <link>https://arxiv.org/abs/2601.16964v1</link>
  <guid>https://arxiv.org/abs/2601.16964v1</guid>
  <pubDate>Fri, 23 Jan 2026 18:33:41 +0000</pubDate>
  <description>arXiv cs.AI - The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark data</description>
</item>

<item>
  <title>Evaluating Large Vision-language Models for Surgical Tool Detection</title>
  <link>https://arxiv.org/abs/2601.16895v1</link>
  <guid>https://arxiv.org/abs/2601.16895v1</guid>
  <pubDate>Fri, 23 Jan 2026 17:00:46 +0000</pubDate>
  <description>arXiv cs.AI - Surgery is a highly complex process, and artificial intelligence has emerged as a transformative force in supporting surgical guidance and decision-making. However, the unimodal nature of most current AI systems limits their ability to achieve a holistic understanding of surgical workflows. This highlights the need for general-purpose surgical AI systems capable of comprehensively modeling the int</description>
</item>

<item>
  <title>MAGE-KT: Multi-Agent Graph-Enhanced Knowledge Tracing with Subgraph Retrieval and Asymmetric Fusion</title>
  <link>https://arxiv.org/abs/2601.16886v1</link>
  <guid>https://arxiv.org/abs/2601.16886v1</guid>
  <pubDate>Fri, 23 Jan 2026 16:51:08 +0000</pubDate>
  <description>arXiv cs.AI - Knowledge Tracing (KT) aims to model a student&apos;s learning trajectory and predict performance on the next question. A key challenge is how to better represent the relationships among students, questions, and knowledge concepts (KCs). Recently, graph-based KT paradigms have shown promise for this problem. However, existing methods have not sufficiently explored inter-concept relations, often inferre</description>
</item>

</channel>
</rss>
